{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "wYxGNJCM5vw7",
        "tzBprG7mGXld",
        "KyR3XplKPQ2T",
        "cK_IIaV9btCS"
      ],
      "authorship_tag": "ABX9TyPtAdIoDYt+knMmjyNluast",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Montrealboyz/EQ_Blast_discriminate/blob/main/Baseline_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSAoXdWaxUKA",
        "outputId": "0678e56c-dc17-4be8-bfc4-076e4d674f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.11)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.26.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision --upgrade\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.utils import save_image\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import TensorDataset\n",
        "from google.colab import drive\n",
        "import torch.optim as optim\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"Nvidia Cuda/GPU is available!\")\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive' )\n",
        "os.chdir('/content/gdrive/MyDrive/AGU_discrimination') # change path if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfj6y__92wbk",
        "outputId": "eb0444d7-ea9f-4e01-cecc-e496fe768baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nvidia Cuda/GPU is available!\n",
            "Fri Nov 29 00:07:50 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   45C    P8              13W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for the training data\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the images\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip images horizontally\n",
        "    transforms.RandomRotation(10),  # Randomly rotate images by up to 10 degrees\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize images\n",
        "])\n",
        "\n",
        "# Transformations for the validation and testing data\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the images\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize images\n",
        "])\n",
        "\n",
        "\n",
        "file_path = './blast_tensor_SNR18.pt'\n",
        "blast_tensor = torch.load(file_path)\n",
        "file_path = './eq_tensor_SNR18.pt'\n",
        "eq_tensor = torch.load(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCdpwFwD2z3t",
        "outputId": "78ddb406-bca2-4c46-acbd-542ee37f6787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-69d4466866ca>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  blast_tensor = torch.load(file_path)\n",
            "<ipython-input-3-69d4466866ca>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  eq_tensor = torch.load(file_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = eq_tensor.size(0)\n",
        "eq_labels = np.ones((num_samples, 1))\n",
        "num_samples =blast_tensor.size(0)\n",
        "blast_labels = np.zeros((num_samples, 1))\n",
        "blast_hour = np.load('blast_tensor_SNR18_hour.npy')\n",
        "eq_hour = np.load('eq_tensor_SNR18_hour.npy')\n",
        "eq_labels = torch.from_numpy(eq_labels).float()\n",
        "blast_labels = torch.from_numpy(blast_labels).float()\n",
        "eq_tensor = eq_tensor.float()\n",
        "blast_tensor = blast_tensor.float()\n",
        "data = torch.cat((eq_tensor, blast_tensor), dim=0)\n",
        "labels = torch.cat((eq_labels, blast_labels), dim=0)\n",
        "# Combine the hour data similarly to how you combined the images and labels\n",
        "hours = np.concatenate((eq_hour, blast_hour), axis=0)\n",
        "hours = torch.from_numpy(hours).float()  # Convert to a PyTorch tensor\n",
        "hours = hours.unsqueeze(1)"
      ],
      "metadata": {
        "id": "01PrTP-k39qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, images, labels, hours, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.hours = hours\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        hour = self.hours[idx]  # Retrieve the corresponding hour\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, hour  # Return the image, its corresponding hour, and label"
      ],
      "metadata": {
        "id": "1kU9MIbY4B9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_split_data(data, labels, hours, train_frac=0.7, val_frac=0.15):\n",
        "    # Shuffle the data, labels, and hours together\n",
        "    perm = torch.randperm(data.size(0))\n",
        "    data_shuffled = data[perm]\n",
        "    labels_shuffled = labels[perm]\n",
        "    hours_shuffled = hours[perm]  # Shuffle hours with the same permutation\n",
        "\n",
        "    # Split the data\n",
        "    train_size = int(data.size(0) * train_frac)\n",
        "    val_size = int(data.size(0) * val_frac)\n",
        "\n",
        "    train_data = data_shuffled[:train_size]\n",
        "    train_labels = labels_shuffled[:train_size]\n",
        "    train_hours = hours_shuffled[:train_size]  # Split hours for training\n",
        "\n",
        "    val_data = data_shuffled[train_size:train_size + val_size]\n",
        "    val_labels = labels_shuffled[train_size:train_size + val_size]\n",
        "    val_hours = hours_shuffled[train_size:train_size + val_size]  # Split hours for validation\n",
        "\n",
        "    test_data = data_shuffled[train_size + val_size:]\n",
        "    test_labels = labels_shuffled[train_size + val_size:]\n",
        "    test_hours = hours_shuffled[train_size + val_size:]  # Split hours for testing\n",
        "\n",
        "    return train_data, train_labels, train_hours, val_data, val_labels, val_hours, test_data, test_labels, test_hours"
      ],
      "metadata": {
        "id": "RqU5enkD4EUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the function call to include `hours`\n",
        "train_data, train_labels, train_hours, val_data, val_labels, val_hours, test_data, test_labels,test_hours = shuffle_split_data(data, labels, hours)  # Add 'hours' splitting here"
      ],
      "metadata": {
        "id": "sght_3Y14VeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Adjust as per your requirement\n",
        "\n",
        "# Assuming you have train_hours, val_hours, and test_hours defined from your data splitting process\n",
        "train_dataset = CustomDataset(train_data, train_labels, train_hours, transform=train_transforms)\n",
        "val_dataset = CustomDataset(val_data, val_labels, val_hours, transform=test_transforms)\n",
        "test_dataset = CustomDataset(test_data, test_labels, test_hours, transform=test_transforms)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fKcz-S_34YJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Resnet-50**"
      ],
      "metadata": {
        "id": "qAyUUl-g-WYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50WithHours(nn.Module):\n",
        "    def __init__(self, num_hour_features=1, dropout_prob=0):\n",
        "        super(ResNet50WithHours, self).__init__()\n",
        "        # Load a pre-trained ResNet-50 model\n",
        "        self.resnet50 = models.resnet50(pretrained=True)\n",
        "        # Remove the final fully connected layer\n",
        "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-1])\n",
        "\n",
        "        # Update for ResNet-50\n",
        "        self.num_ftrs = 2048  # Output features from ResNet-50\n",
        "\n",
        "        # Additional layer for hour feature(s)\n",
        "        self.hour_fc = nn.Linear(num_hour_features, 24)  # Transform the hour feature\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout_resnet = nn.Dropout(p=dropout_prob)\n",
        "        self.dropout_hour = nn.Dropout(p=dropout_prob)\n",
        "        self.dropout_combined = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        # Final layer for binary classification\n",
        "        self.final_fc = nn.Linear(self.num_ftrs + 24, 1)\n",
        "\n",
        "    def forward(self, images, hours):\n",
        "        # Forward pass through ResNet-50\n",
        "        resnet_features = self.resnet50(images)\n",
        "        resnet_features = resnet_features.view(-1, self.num_ftrs)  # Flatten the features\n",
        "        resnet_features = self.dropout_resnet(resnet_features)  # Apply dropout\n",
        "\n",
        "        # Forward pass through the hour feature layer\n",
        "        hour_features = torch.relu(self.hour_fc(hours))\n",
        "        hour_features = self.dropout_hour(hour_features)  # Apply dropout\n",
        "\n",
        "        # Combine the features\n",
        "        combined_features = torch.cat((resnet_features, hour_features), dim=1)\n",
        "        combined_features = self.dropout_combined(combined_features)  # Apply dropout\n",
        "\n",
        "        # Final classification\n",
        "        output = self.final_fc(combined_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "AR6tkTn0-axn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.01"
      ],
      "metadata": {
        "id": "ah6nIgir-mPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50WithHours(num_hour_features=1, dropout_prob=0).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enYo2c_R-jip",
        "outputId": "b4eb157f-26d1-4f3c-8a74-f5e199ac0381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 210MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_xYVX60--f7",
        "outputId": "eccc6bdc-0aba-46e9-9b63-9e60054b600a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.24730832874774933, Accuracy: 88.56037816931672%\n",
            "Iteration: 1000, Loss: 0.1337786465883255, Accuracy: 91.72324881822088%\n",
            "Iteration: 1500, Loss: 0.25920796394348145, Accuracy: 92.7632144391921%\n",
            "Iteration: 2000, Loss: 0.19757899641990662, Accuracy: 93.02965191233348%\n",
            "Iteration: 2500, Loss: 0.21460166573524475, Accuracy: 91.4568113450795%\n",
            "Iteration: 3000, Loss: 0.2411913424730301, Accuracy: 92.97808336914483%\n",
            "Iteration: 3500, Loss: 0.14584645628929138, Accuracy: 92.51396648044692%\n",
            "Iteration: 4000, Loss: 0.14501284062862396, Accuracy: 94.23291792006876%\n",
            "Iteration: 4500, Loss: 0.1493687778711319, Accuracy: 93.60550064460679%\n",
            "Iteration: 5000, Loss: 0.13278797268867493, Accuracy: 91.67168027503223%\n",
            "Iteration: 5500, Loss: 0.08884447813034058, Accuracy: 94.3704340352385%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYx_58RD_Cej",
        "outputId": "93294564-1920-45ab-f8a7-fe49db7608f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1546\n",
            "Accuracy: 0.9421\n",
            "Precision: 0.9384\n",
            "Recall: 0.9579\n",
            "F1 Score: 0.9480\n",
            "ROC AUC Score: 0.9817\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLDMlUW__GF0",
        "outputId": "fa8dd8c7-41c1-4f95-94bd-ad63c25eb2b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 472848896 bytes\n",
            "Memory Reserved after model1 deletion: 899678208 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.001"
      ],
      "metadata": {
        "id": "OioNJzAm_OXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50WithHours(num_hour_features=1, dropout_prob=0).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQO6gnJM_a82",
        "outputId": "44b66efa-a99d-4ace-fabb-4ec5a2d53b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvjnZXaX_jD4",
        "outputId": "6a68c1c4-7f76-4f78-834d-90442528b4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.24691146612167358, Accuracy: 93.21014181349376%\n",
            "Iteration: 1000, Loss: 0.11035985499620438, Accuracy: 93.67425870219166%\n",
            "Iteration: 1500, Loss: 0.197535440325737, Accuracy: 94.15556510528577%\n",
            "Iteration: 2000, Loss: 0.15805745124816895, Accuracy: 94.91190373871937%\n",
            "Iteration: 2500, Loss: 0.2803337275981903, Accuracy: 94.42200257842715%\n",
            "Iteration: 3000, Loss: 0.07939139753580093, Accuracy: 94.92049849591749%\n",
            "Iteration: 3500, Loss: 0.05122937262058258, Accuracy: 94.81736140954018%\n",
            "Iteration: 4000, Loss: 0.12070658802986145, Accuracy: 95.22131499785131%\n",
            "Iteration: 4500, Loss: 0.16030585765838623, Accuracy: 95.25569402664375%\n",
            "Iteration: 5000, Loss: 0.22791031002998352, Accuracy: 95.60807907176623%\n",
            "Iteration: 5500, Loss: 0.16496744751930237, Accuracy: 95.53072625698324%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDU8Xj8m_oOo",
        "outputId": "17ad9196-b6ad-4e2e-abf0-f36f7b1ef46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1459\n",
            "Accuracy: 0.9366\n",
            "Precision: 0.9754\n",
            "Recall: 0.9078\n",
            "F1 Score: 0.9403\n",
            "ROC AUC Score: 0.9857\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "id": "k2w6NjA4_vjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.0001"
      ],
      "metadata": {
        "id": "yeXAJjJv_TAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50WithHours(num_hour_features=1, dropout_prob=0).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5h6mU5x_ch-",
        "outputId": "4f0cc266-0f06-43c4-9dbf-699de494bf2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 217MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOYUbrBE_k3_",
        "outputId": "afb1cccd-9ea5-493d-feee-0a732f6edf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.14199517667293549, Accuracy: 94.05242801890847%\n",
            "Iteration: 1000, Loss: 0.16648559272289276, Accuracy: 95.00644606789858%\n",
            "Iteration: 1500, Loss: 0.14333130419254303, Accuracy: 95.68543188654921%\n",
            "Iteration: 2000, Loss: 0.16717742383480072, Accuracy: 95.79716373012462%\n",
            "Iteration: 2500, Loss: 0.16665399074554443, Accuracy: 95.68543188654921%\n",
            "Iteration: 3000, Loss: 0.0505056194961071, Accuracy: 95.99484314568113%\n",
            "Iteration: 3500, Loss: 0.19950005412101746, Accuracy: 96.08079071766223%\n",
            "Iteration: 4000, Loss: 0.12164194881916046, Accuracy: 95.26428878384186%\n",
            "Iteration: 4500, Loss: 0.14899513125419617, Accuracy: 95.89170605930383%\n",
            "Iteration: 5000, Loss: 0.056694015860557556, Accuracy: 96.21830683283197%\n",
            "Iteration: 5500, Loss: 0.09944994747638702, Accuracy: 96.38160721959605%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "id": "nrA5TWsn_pa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19e14609-61a7-4001-fa72-00403ef203d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1021\n",
            "Accuracy: 0.9631\n",
            "Precision: 0.9696\n",
            "Recall: 0.9634\n",
            "F1 Score: 0.9665\n",
            "ROC AUC Score: 0.9914\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "id": "9I-u1g_9_yB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9ce150-ef74-4849-f269-54219ba42840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 473935872 bytes\n",
            "Memory Reserved after model1 deletion: 803209216 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.00001"
      ],
      "metadata": {
        "id": "OwGL-od1_Vxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50WithHours(num_hour_features=1, dropout_prob=0).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "sxyzZ6GU_d-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624c303c-873c-4b73-b8df-6b1f6847c030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r8jYz0e_j7v",
        "outputId": "535e347d-f62a-4e8a-e6de-c15a1aec65da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.12193170189857483, Accuracy: 91.24194241512677%\n",
            "Iteration: 1000, Loss: 0.09444105625152588, Accuracy: 92.60850880962613%\n",
            "Iteration: 1500, Loss: 0.06555213034152985, Accuracy: 93.82896433175763%\n",
            "Iteration: 2000, Loss: 0.17121830582618713, Accuracy: 93.69144821658789%\n",
            "Iteration: 2500, Loss: 0.0651993528008461, Accuracy: 94.32746024924796%\n",
            "Iteration: 3000, Loss: 0.057550884783267975, Accuracy: 94.23291792006876%\n",
            "Iteration: 3500, Loss: 0.0912308469414711, Accuracy: 94.86033519553072%\n",
            "Iteration: 4000, Loss: 0.026316873729228973, Accuracy: 94.8259561667383%\n",
            "Iteration: 4500, Loss: 0.10558198392391205, Accuracy: 94.80876665234207%\n",
            "Iteration: 5000, Loss: 0.1299992799758911, Accuracy: 94.8259561667383%\n",
            "Iteration: 5500, Loss: 0.016605194658041, Accuracy: 94.96347228190804%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "id": "h3RAO3MC_qpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce167db-4cff-4584-fcec-d5037509502a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1139\n",
            "Accuracy: 0.9580\n",
            "Precision: 0.9660\n",
            "Recall: 0.9575\n",
            "F1 Score: 0.9617\n",
            "ROC AUC Score: 0.9888\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpjVUAMX_zFw",
        "outputId": "ae8e4d5c-cf99-4316-943c-437fda5b6811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 438398976 bytes\n",
            "Memory Reserved after model1 deletion: 692060160 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Resnet-18**"
      ],
      "metadata": {
        "id": "wYxGNJCM5vw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetWithHours(nn.Module):\n",
        "    def __init__(self, num_hour_features=1):\n",
        "        super(ResNetWithHours, self).__init__()\n",
        "        # Load a pre-trained ResNet-18 model\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "        # Remove the final fully connected layer\n",
        "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])\n",
        "\n",
        "        self.num_ftrs = 512  # Updated for ResNet-18\n",
        "\n",
        "        # Additional layer for hour feature(s)\n",
        "        self.hour_fc = nn.Linear(num_hour_features, 16)  # Example transformation of the hour feature\n",
        "\n",
        "        # Final layer for binary classification\n",
        "        self.final_fc = nn.Linear(self.num_ftrs + 16, 1)  # Use self.num_ftrs here\n",
        "\n",
        "    def forward(self, images, hours):\n",
        "        # Forward pass through ResNet-18\n",
        "        resnet_features = self.resnet18(images)\n",
        "        resnet_features = resnet_features.view(-1, self.num_ftrs)  # Flatten the features\n",
        "\n",
        "        # Forward pass through the hour feature layer\n",
        "        hour_features = torch.relu(self.hour_fc(hours))\n",
        "\n",
        "        # Combine the features\n",
        "        combined_features = torch.cat((resnet_features, hour_features), dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        output = self.final_fc(combined_features)\n",
        "        return output"
      ],
      "metadata": {
        "id": "niNX9cdC5q8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.01"
      ],
      "metadata": {
        "id": "Fb4ZYyWDNIf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H30CiV3s6oao",
        "outputId": "e48a7fe1-12c7-4fc4-8a18-b3a03250b221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 224MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMIIlYuS62GY",
        "outputId": "b427b18c-69d0-402d-b8c9-2a020151fe50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.22581788897514343, Accuracy: 85.87021916630856%\n",
            "Iteration: 1000, Loss: 0.2912416160106659, Accuracy: 88.6291362269016%\n",
            "Iteration: 1500, Loss: 0.1125142052769661, Accuracy: 91.42243231628707%\n",
            "Iteration: 2000, Loss: 0.14540427923202515, Accuracy: 92.78899871078642%\n",
            "Iteration: 2500, Loss: 0.13263902068138123, Accuracy: 92.22174473571121%\n",
            "Iteration: 3000, Loss: 0.18058906495571136, Accuracy: 93.63987967339922%\n",
            "Iteration: 3500, Loss: 0.200923353433609, Accuracy: 94.0008594757198%\n",
            "Iteration: 4000, Loss: 0.09421859681606293, Accuracy: 94.02664374731414%\n",
            "Iteration: 4500, Loss: 0.13032330572605133, Accuracy: 92.7632144391921%\n",
            "Iteration: 5000, Loss: 0.12590914964675903, Accuracy: 94.98925655350236%\n",
            "Iteration: 5500, Loss: 0.1229606345295906, Accuracy: 94.68844005156855%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smf8E4yG7C-B",
        "outputId": "11d1f2c6-354d-4297-ed05-31e7f44d8f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1587\n",
            "Accuracy: 0.9394\n",
            "Precision: 0.9214\n",
            "Recall: 0.9742\n",
            "F1 Score: 0.9471\n",
            "ROC AUC Score: 0.9850\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70y_RCyr84nB",
        "outputId": "5b38deeb-6117-40a1-f28d-e06c9d37365f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 227232256 bytes\n",
            "Memory Reserved after model1 deletion: 371195904 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.001"
      ],
      "metadata": {
        "id": "Gsu8Lq1sNPew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMGo4EW4-EPu",
        "outputId": "5d7453f4-f17f-4390-88a6-a5714c3f06e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75aIJz_QNF1v",
        "outputId": "25b15128-b6e3-4f3d-adb3-b7df50c5d1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.05217624455690384, Accuracy: 93.45938977223894%\n",
            "Iteration: 1000, Loss: 0.24652116000652313, Accuracy: 93.59690588740868%\n",
            "Iteration: 1500, Loss: 0.1710663139820099, Accuracy: 94.91190373871937%\n",
            "Iteration: 2000, Loss: 0.09191072732210159, Accuracy: 94.97206703910615%\n",
            "Iteration: 2500, Loss: 0.1600845605134964, Accuracy: 95.22990975504942%\n",
            "Iteration: 3000, Loss: 0.10231349617242813, Accuracy: 95.05801461108723%\n",
            "Iteration: 3500, Loss: 0.08674722909927368, Accuracy: 95.13536742587021%\n",
            "Iteration: 4000, Loss: 0.1464272439479828, Accuracy: 95.3932101418135%\n",
            "Iteration: 4500, Loss: 0.12844708561897278, Accuracy: 95.37602062741728%\n",
            "Iteration: 5000, Loss: 0.1259939968585968, Accuracy: 95.18693596905888%\n",
            "Iteration: 5500, Loss: 0.11111168563365936, Accuracy: 95.8745165449076%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYT8Tx3qNZ54",
        "outputId": "0a136a99-80bd-4c10-c2fb-3b96369cfa1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1121\n",
            "Accuracy: 0.9562\n",
            "Precision: 0.9709\n",
            "Recall: 0.9498\n",
            "F1 Score: 0.9602\n",
            "ROC AUC Score: 0.9895\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhTDUrwjQ88C",
        "outputId": "b4431c62-592e-4a36-d16e-90d50b16f0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 266169856 bytes\n",
            "Memory Reserved after model1 deletion: 417333248 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.0001"
      ],
      "metadata": {
        "id": "c95Q8EiOQ14i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyReyH6RRyp2",
        "outputId": "b59d5f16-37ae-49a7-fa65-77c4fa5b0629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE71JfAjR4uH",
        "outputId": "9f9b2115-fa5a-4874-83c0-e78c173ac25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.097661592066288, Accuracy: 92.25612376450366%\n",
            "Iteration: 1000, Loss: 0.1474466621875763, Accuracy: 95.25569402664375%\n",
            "Iteration: 1500, Loss: 0.07751607149839401, Accuracy: 95.46196819939837%\n",
            "Iteration: 2000, Loss: 0.040054336190223694, Accuracy: 95.57370004297378%\n",
            "Iteration: 2500, Loss: 0.04979481175541878, Accuracy: 95.81435324452085%\n",
            "Iteration: 3000, Loss: 0.12764373421669006, Accuracy: 96.07219596046411%\n",
            "Iteration: 3500, Loss: 0.1401008665561676, Accuracy: 96.063601203266%\n",
            "Iteration: 4000, Loss: 0.22898411750793457, Accuracy: 96.27847013321873%\n",
            "Iteration: 4500, Loss: 0.15430347621440887, Accuracy: 95.93467984529437%\n",
            "Iteration: 5000, Loss: 0.07680060714483261, Accuracy: 95.68543188654921%\n",
            "Iteration: 5500, Loss: 0.028390103951096535, Accuracy: 96.12376450365277%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXyWce7KSE04",
        "outputId": "a35721a3-8bde-4599-bbf5-e7c312866034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1359\n",
            "Accuracy: 0.9427\n",
            "Precision: 0.9853\n",
            "Recall: 0.9105\n",
            "F1 Score: 0.9464\n",
            "ROC AUC Score: 0.9901\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLWBmwh_SHvo",
        "outputId": "e9efb893-7d1d-4e40-ce2e-d3565add37b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 227828224 bytes\n",
            "Memory Reserved after model1 deletion: 463470592 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.00001"
      ],
      "metadata": {
        "id": "7EMGxT3bSMFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbfvRIQISIeY",
        "outputId": "31979f26-45f7-44c9-a96e-aecd7588abb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_7uhIZFSQUw",
        "outputId": "5104112d-4f26-46ae-add0-7a78c4ffd442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.15535256266593933, Accuracy: 89.57455951869359%\n",
            "Iteration: 1000, Loss: 0.22010602056980133, Accuracy: 91.41383755908896%\n",
            "Iteration: 1500, Loss: 0.2211778163909912, Accuracy: 92.40223463687151%\n",
            "Iteration: 2000, Loss: 0.20367984473705292, Accuracy: 93.08981521272024%\n",
            "Iteration: 2500, Loss: 0.18046337366104126, Accuracy: 93.53674258702192%\n",
            "Iteration: 3000, Loss: 0.09347458183765411, Accuracy: 93.55393210141814%\n",
            "Iteration: 3500, Loss: 0.09028075635433197, Accuracy: 94.06961753330468%\n",
            "Iteration: 4000, Loss: 0.10890308767557144, Accuracy: 94.03523850451225%\n",
            "Iteration: 4500, Loss: 0.08687527477741241, Accuracy: 94.36183927804039%\n",
            "Iteration: 5000, Loss: 0.07994537800550461, Accuracy: 94.7400085947572%\n",
            "Iteration: 5500, Loss: 0.06215696781873703, Accuracy: 94.94628276751182%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2mpuk-wSTzJ",
        "outputId": "a9830d77-2a88-4de2-883d-2a75274ec814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1222\n",
            "Accuracy: 0.9562\n",
            "Precision: 0.9553\n",
            "Recall: 0.9666\n",
            "F1 Score: 0.9609\n",
            "ROC AUC Score: 0.9867\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCZThPXlSYp5",
        "outputId": "73b5fd73-0a15-4083-9279-37c0f37604d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 227558912 bytes\n",
            "Memory Reserved after model1 deletion: 497025024 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VGG-16**"
      ],
      "metadata": {
        "id": "tzBprG7mGXld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16WithHours(nn.Module):\n",
        "    def __init__(self, num_hour_features=1):\n",
        "        super(VGG16WithHours, self).__init__()\n",
        "        # Load a pre-trained VGG16 model\n",
        "        self.vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "        # Modify the classifier\n",
        "        # Remove the last classification layer (the last Linear layer)\n",
        "        self.vgg16.classifier = nn.Sequential(\n",
        "            *list(self.vgg16.classifier.children())[:-1]  # Exclude the last layer\n",
        "        )\n",
        "\n",
        "        # Find the number of features output by the last Linear layer\n",
        "        num_ftrs = None\n",
        "        for layer in reversed(self.vgg16.classifier):\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                num_ftrs = layer.out_features\n",
        "                break\n",
        "        if num_ftrs is None:\n",
        "            raise ValueError(\"No Linear layer found in vgg16.classifier\")\n",
        "\n",
        "        # Additional layer for hour feature(s)\n",
        "        self.hour_fc = nn.Linear(num_hour_features, 16)  # Transform the hour feature\n",
        "\n",
        "        # Final layer for binary classification\n",
        "        self.final_fc = nn.Linear(num_ftrs + 16, 1)\n",
        "\n",
        "    def forward(self, images, hours):\n",
        "        # Forward pass through VGG16\n",
        "        features = self.vgg16(images)  # Output size: [batch_size, num_ftrs]\n",
        "\n",
        "        # Forward pass through the hour feature layer\n",
        "        hour_features = torch.relu(self.hour_fc(hours))  # Output size: [batch_size, 16]\n",
        "\n",
        "        # Combine the features\n",
        "        combined_features = torch.cat((features, hour_features), dim=1)  # [batch_size, num_ftrs + 16]\n",
        "\n",
        "        # Final classification\n",
        "        output = self.final_fc(combined_features)  # Output size: [batch_size, 1]\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "CcHjQehyTdLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.01"
      ],
      "metadata": {
        "id": "16jaoBEpLhWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Move the model to the appropriate device\n",
        "model = VGG16WithHours(num_hour_features=1).to(device)\n",
        "\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "t_t9Ju23Gbm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqqdPV7OLSiM",
        "outputId": "7f7da37f-048a-4199-fae9-dc2ff2f8bca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.4277605414390564, Accuracy: 77.06059303824667%\n",
            "Iteration: 1000, Loss: 0.39756089448928833, Accuracy: 78.03180060163301%\n",
            "Iteration: 1500, Loss: 0.3726741671562195, Accuracy: 76.9058874086807%\n",
            "Iteration: 2000, Loss: 0.44388020038604736, Accuracy: 76.95745595186936%\n",
            "Iteration: 2500, Loss: 0.40823936462402344, Accuracy: 78.03180060163301%\n",
            "Iteration: 3000, Loss: 0.5207967758178711, Accuracy: 76.95745595186936%\n",
            "Iteration: 3500, Loss: 0.48589831590652466, Accuracy: 76.95745595186936%\n",
            "Iteration: 4000, Loss: 0.46096891164779663, Accuracy: 80.48990116029222%\n",
            "Iteration: 4500, Loss: 0.3895186185836792, Accuracy: 80.6532015470563%\n",
            "Iteration: 5000, Loss: 0.42466872930526733, Accuracy: 79.51869359690589%\n",
            "Iteration: 5500, Loss: 0.3910786807537079, Accuracy: 81.74473571121615%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBJSaqkaL5CX",
        "outputId": "1b84fce2-f89c-4a4a-b949-55e024e29d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.3978\n",
            "Accuracy: 0.8030\n",
            "Precision: 0.8648\n",
            "Recall: 0.7589\n",
            "F1 Score: 0.8084\n",
            "ROC AUC Score: 0.8651\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZilzMF3L7Im",
        "outputId": "431c603f-73cd-4a25-fc21-983e844c8f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 2197217792 bytes\n",
            "Memory Reserved after model1 deletion: 3093299200 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.001"
      ],
      "metadata": {
        "id": "1fJ-hAX_aTHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Move the model to the appropriate device\n",
        "model = VGG16WithHours(num_hour_features=1).to(device)\n",
        "\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBEq74ljadVr",
        "outputId": "8ba19b2b-9611-4254-b688-c3479efe7db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfu9PCVoahKz",
        "outputId": "aaff46d9-725d-4b64-9269-5d3ba433cbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.42564404010772705, Accuracy: 81.35797163730125%\n",
            "Iteration: 1000, Loss: 0.11262533068656921, Accuracy: 84.1340782122905%\n",
            "Iteration: 1500, Loss: 0.21463292837142944, Accuracy: 93.18435754189944%\n",
            "Iteration: 2000, Loss: 0.15441660583019257, Accuracy: 92.38504512247529%\n",
            "Iteration: 2500, Loss: 0.15930379927158356, Accuracy: 93.78599054576708%\n",
            "Iteration: 3000, Loss: 0.3134002089500427, Accuracy: 93.10700472711646%\n",
            "Iteration: 3500, Loss: 0.18326322734355927, Accuracy: 93.55393210141814%\n",
            "Iteration: 4000, Loss: 0.09494480490684509, Accuracy: 94.74860335195531%\n",
            "Iteration: 4500, Loss: 0.22549748420715332, Accuracy: 93.19295229909756%\n",
            "Iteration: 5000, Loss: 0.14156734943389893, Accuracy: 94.80876665234207%\n",
            "Iteration: 5500, Loss: 0.15190403163433075, Accuracy: 93.5195530726257%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIY5YcBPbotK",
        "outputId": "4ea99bbf-4897-412d-c429-bee95e08a621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1578\n",
            "Accuracy: 0.9451\n",
            "Precision: 0.9336\n",
            "Recall: 0.9686\n",
            "F1 Score: 0.9508\n",
            "ROC AUC Score: 0.9855\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNo6tHO5bqfx",
        "outputId": "0ac3aba9-7f57-4fcb-de9d-41e46ee6c2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 2196649472 bytes\n",
            "Memory Reserved after model1 deletion: 2980052992 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.0001"
      ],
      "metadata": {
        "id": "THLQn_SxbwTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Move the model to the appropriate device\n",
        "model = VGG16WithHours(num_hour_features=1).to(device)\n",
        "\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXjQ8lfHb1Kh",
        "outputId": "b7dddca9-ae85-4daf-cff9-db03175e31bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJz7IPNqb6GE",
        "outputId": "ca40a07d-815a-41ac-a76b-1e16bb81d9ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.13139332830905914, Accuracy: 94.30167597765363%\n",
            "Iteration: 1000, Loss: 0.1117510199546814, Accuracy: 94.5509239363988%\n",
            "Iteration: 1500, Loss: 0.06414756178855896, Accuracy: 95.45337344220026%\n",
            "Iteration: 2000, Loss: 0.0693783387541771, Accuracy: 94.94628276751182%\n",
            "Iteration: 2500, Loss: 0.11553597450256348, Accuracy: 95.01504082509669%\n",
            "Iteration: 3000, Loss: 0.08226904273033142, Accuracy: 95.97765363128492%\n",
            "Iteration: 3500, Loss: 0.034720323979854584, Accuracy: 95.23850451224753%\n",
            "Iteration: 4000, Loss: 0.0774001032114029, Accuracy: 96.17533304684143%\n",
            "Iteration: 4500, Loss: 0.06056244671344757, Accuracy: 96.05500644606789%\n",
            "Iteration: 5000, Loss: 0.05751349404454231, Accuracy: 96.2440911044263%\n",
            "Iteration: 5500, Loss: 0.0410083532333374, Accuracy: 96.02062741727546%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCmPFWQsb9ca",
        "outputId": "c11256c5-ff21-4de3-9ceb-6fa36cc871d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1219\n",
            "Accuracy: 0.9498\n",
            "Precision: 0.9830\n",
            "Recall: 0.9243\n",
            "F1 Score: 0.9528\n",
            "ROC AUC Score: 0.9904\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewzbVukqb_ka",
        "outputId": "bb4093ab-bcdf-40f7-f4de-b83aac085e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 2196663808 bytes\n",
            "Memory Reserved after model1 deletion: 2980052992 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.00001"
      ],
      "metadata": {
        "id": "Vydo-Gy4cAQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Move the model to the appropriate device\n",
        "model = VGG16WithHours(num_hour_features=1).to(device)\n",
        "\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzaX5sjTcOTb",
        "outputId": "3488a8cd-2df4-4512-b90e-09dc68b2177a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlObX9nfcSBM",
        "outputId": "1db068ce-c4fd-4b08-c330-4fa9e7db6167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.2810930609703064, Accuracy: 93.22733132788998%\n",
            "Iteration: 1000, Loss: 0.19488099217414856, Accuracy: 94.06961753330468%\n",
            "Iteration: 1500, Loss: 0.16232308745384216, Accuracy: 94.68844005156855%\n",
            "Iteration: 2000, Loss: 0.15970219671726227, Accuracy: 94.88611946712506%\n",
            "Iteration: 2500, Loss: 0.14172379672527313, Accuracy: 95.19553072625699%\n",
            "Iteration: 3000, Loss: 0.079299196600914, Accuracy: 95.41899441340782%\n",
            "Iteration: 3500, Loss: 0.05168115720152855, Accuracy: 95.51353674258702%\n",
            "Iteration: 4000, Loss: 0.08777405321598053, Accuracy: 95.60807907176623%\n",
            "Iteration: 4500, Loss: 0.09047064185142517, Accuracy: 95.79716373012462%\n",
            "Iteration: 5000, Loss: 0.050328612327575684, Accuracy: 95.71121615814353%\n",
            "Iteration: 5500, Loss: 0.13508306443691254, Accuracy: 95.54791577137946%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy3w-7g8cUVk",
        "outputId": "0466b422-af95-438c-db3e-8caa85a8290b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1132\n",
            "Accuracy: 0.9574\n",
            "Precision: 0.9751\n",
            "Recall: 0.9465\n",
            "F1 Score: 0.9606\n",
            "ROC AUC Score: 0.9897\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOAHnT6-cXCk",
        "outputId": "791ed2e4-cba0-4b30-8cde-34285dcd066b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 2196647936 bytes\n",
            "Memory Reserved after model1 deletion: 3093299200 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EfficientNet**"
      ],
      "metadata": {
        "id": "KyR3XplKPQ2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNetWithHours(nn.Module):\n",
        "    def __init__(self, num_hour_features=1):\n",
        "        super(EfficientNetWithHours, self).__init__()\n",
        "        # Load a pre-trained EfficientNet-B0 model\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Extract the feature extractor part\n",
        "        self.features = self.efficientnet.features  # EfficientNet feature extractor\n",
        "\n",
        "        # The number of features output by the feature extractor\n",
        "        num_ftrs = self.efficientnet.classifier[1].in_features  # Should be 1280\n",
        "\n",
        "        # Additional layer for hour feature(s)\n",
        "        self.hour_fc = nn.Linear(num_hour_features, 16)  # Transform the hour feature\n",
        "\n",
        "        # Define a new classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(num_ftrs + 16, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, images, hours):\n",
        "        # Forward pass through EfficientNet feature extractor\n",
        "        features = self.features(images)\n",
        "        features = features.mean([2, 3])  # Global average pooling\n",
        "        # features size: [batch_size, num_ftrs]\n",
        "\n",
        "        # Forward pass through the hour feature layer\n",
        "        hour_features = torch.relu(self.hour_fc(hours))  # Output size: [batch_size, 16]\n",
        "\n",
        "        # Combine the features\n",
        "        combined_features = torch.cat((features, hour_features), dim=1)  # [batch_size, num_ftrs + 16]\n",
        "\n",
        "        # Final classification\n",
        "        output = self.classifier(combined_features)  # Output size: [batch_size, 1]\n",
        "        return output"
      ],
      "metadata": {
        "id": "I247dZ_lQdpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.01"
      ],
      "metadata": {
        "id": "DSUKpMDkSdZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EfficientNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB-0MHZLSJHD",
        "outputId": "cc4e9110-ea5d-41e9-dd78-6cdcc0076a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M39138pzSmGl",
        "outputId": "221fa080-ed83-4596-e1cb-87149b495895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.13078011572360992, Accuracy: 92.60850880962613%\n",
            "Iteration: 1000, Loss: 0.24574053287506104, Accuracy: 93.01246239793726%\n",
            "Iteration: 1500, Loss: 0.17433631420135498, Accuracy: 93.78599054576708%\n",
            "Iteration: 2000, Loss: 0.1254563182592392, Accuracy: 93.149978513107%\n",
            "Iteration: 2500, Loss: 0.1805855929851532, Accuracy: 94.40481306403095%\n",
            "Iteration: 3000, Loss: 0.10556334257125854, Accuracy: 94.75719810915342%\n",
            "Iteration: 3500, Loss: 0.1882236897945404, Accuracy: 93.53674258702192%\n",
            "Iteration: 4000, Loss: 0.12700939178466797, Accuracy: 94.19853889127633%\n",
            "Iteration: 4500, Loss: 0.21612408757209778, Accuracy: 94.47357112161582%\n",
            "Iteration: 5000, Loss: 0.11972159147262573, Accuracy: 95.01504082509669%\n",
            "Iteration: 5500, Loss: 0.07452452182769775, Accuracy: 95.01504082509669%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BbKjlf7SuX9",
        "outputId": "01f884f9-a60a-40c2-b567-55d65a3710b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1390\n",
            "Accuracy: 0.9479\n",
            "Precision: 0.9416\n",
            "Recall: 0.9666\n",
            "F1 Score: 0.9540\n",
            "ROC AUC Score: 0.9861\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pF1v6rGSxmt",
        "outputId": "838e3432-4c6d-4099-94a3-cde3991fbec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 117549568 bytes\n",
            "Memory Reserved after model1 deletion: 195035136 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.001"
      ],
      "metadata": {
        "id": "pMRIt9yKS2qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EfficientNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLg1Vt0hS44U",
        "outputId": "ea2ec956-4642-40cf-e6c4-8361c6204784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXqE7ULWS8yW",
        "outputId": "0b9cbf9f-a4e5-46db-b8e1-65461e7fa720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.1284249722957611, Accuracy: 94.86892995272883%\n",
            "Iteration: 1000, Loss: 0.18012598156929016, Accuracy: 95.29007305543618%\n",
            "Iteration: 1500, Loss: 0.09403017908334732, Accuracy: 94.92049849591749%\n",
            "Iteration: 2000, Loss: 0.046612679958343506, Accuracy: 95.70262140094542%\n",
            "Iteration: 2500, Loss: 0.11021242290735245, Accuracy: 96.05500644606789%\n",
            "Iteration: 3000, Loss: 0.03536862134933472, Accuracy: 96.10657498925656%\n",
            "Iteration: 3500, Loss: 0.08426930010318756, Accuracy: 96.11516974645467%\n",
            "Iteration: 4000, Loss: 0.15390725433826447, Accuracy: 96.28706489041684%\n",
            "Iteration: 4500, Loss: 0.10369867086410522, Accuracy: 96.2440911044263%\n",
            "Iteration: 5000, Loss: 0.19862401485443115, Accuracy: 96.39020197679416%\n",
            "Iteration: 5500, Loss: 0.08180643618106842, Accuracy: 96.09798023205845%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhkYon_8TLtW",
        "outputId": "31d0a6df-99bb-4495-f17e-986e1c126d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.0932\n",
            "Accuracy: 0.9653\n",
            "Precision: 0.9742\n",
            "Recall: 0.9634\n",
            "F1 Score: 0.9687\n",
            "ROC AUC Score: 0.9917\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuLzRo4GTOA9",
        "outputId": "9f069b26-4182-4f3a-bc1e-d88d3e746c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 118281728 bytes\n",
            "Memory Reserved after model1 deletion: 255852544 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.0001"
      ],
      "metadata": {
        "id": "yBBxStgLTRog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EfficientNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IODXD6yTWE_",
        "outputId": "3649ee9c-d2af-4675-fb09-c9241199b6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjTeerJVTpbx",
        "outputId": "e3b4433c-eb83-4554-e4ad-a4ee78ae8785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.19425040483474731, Accuracy: 92.72883541039965%\n",
            "Iteration: 1000, Loss: 0.07972130924463272, Accuracy: 94.01804899011603%\n",
            "Iteration: 1500, Loss: 0.17188207805156708, Accuracy: 94.80876665234207%\n",
            "Iteration: 2000, Loss: 0.12325762212276459, Accuracy: 95.0236355822948%\n",
            "Iteration: 2500, Loss: 0.05338066816329956, Accuracy: 95.22990975504942%\n",
            "Iteration: 3000, Loss: 0.1347486972808838, Accuracy: 95.40180489901161%\n",
            "Iteration: 3500, Loss: 0.11200514435768127, Accuracy: 95.82294800171896%\n",
            "Iteration: 4000, Loss: 0.06026378646492958, Accuracy: 95.88311130210572%\n",
            "Iteration: 4500, Loss: 0.1474861055612564, Accuracy: 96.063601203266%\n",
            "Iteration: 5000, Loss: 0.15072090923786163, Accuracy: 95.98624838848302%\n",
            "Iteration: 5500, Loss: 0.05349541828036308, Accuracy: 95.99484314568113%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RDHwsR0TtUw",
        "outputId": "abd7607a-a929-41e0-b2c8-1b2326098558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1022\n",
            "Accuracy: 0.9630\n",
            "Precision: 0.9718\n",
            "Recall: 0.9617\n",
            "F1 Score: 0.9667\n",
            "ROC AUC Score: 0.9912\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPZlX9YDTvsP",
        "outputId": "c5617962-3c3d-47c5-b060-1ddb78b81480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 117510144 bytes\n",
            "Memory Reserved after model1 deletion: 236978176 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.00001"
      ],
      "metadata": {
        "id": "fPTPe4YVT2u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EfficientNetWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.00001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRKXPDP6T5Qa",
        "outputId": "c31f087e-bf51-4c72-877f-63f204ae645c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTeGW_31T99r",
        "outputId": "19b774ce-d37e-4075-f674-c8233afe4071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.3217049241065979, Accuracy: 84.40911044263%\n",
            "Iteration: 1000, Loss: 0.19259116053581238, Accuracy: 89.3940696175333%\n",
            "Iteration: 1500, Loss: 0.24634677171707153, Accuracy: 90.88096261280619%\n",
            "Iteration: 2000, Loss: 0.2866280674934387, Accuracy: 91.80060163300386%\n",
            "Iteration: 2500, Loss: 0.17087391018867493, Accuracy: 92.25612376450366%\n",
            "Iteration: 3000, Loss: 0.10038313269615173, Accuracy: 92.41082939406962%\n",
            "Iteration: 3500, Loss: 0.11389091610908508, Accuracy: 93.09840996991835%\n",
            "Iteration: 4000, Loss: 0.11511431634426117, Accuracy: 93.17576278470133%\n",
            "Iteration: 4500, Loss: 0.06213150918483734, Accuracy: 93.56252685861624%\n",
            "Iteration: 5000, Loss: 0.10357673466205597, Accuracy: 93.78599054576708%\n",
            "Iteration: 5500, Loss: 0.07454939186573029, Accuracy: 93.94929093253116%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY7u2H1tT_2j",
        "outputId": "12dbc85c-3301-4206-8c8a-87291071af37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1291\n",
            "Accuracy: 0.9478\n",
            "Precision: 0.9542\n",
            "Recall: 0.9523\n",
            "F1 Score: 0.9532\n",
            "ROC AUC Score: 0.9862\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdP75SNuUBtq",
        "outputId": "0d8bb15e-6da5-4ac9-8944-91ee519f5914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 156381184 bytes\n",
            "Memory Reserved after model1 deletion: 255852544 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConvNext**"
      ],
      "metadata": {
        "id": "cK_IIaV9btCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ConvNextWithHours(nn.Module):\n",
        "    def __init__(self, num_hour_features=1):\n",
        "        super(ConvNextWithHours, self).__init__()\n",
        "        # Load a pre-trained ConvNeXt-Small model\n",
        "        self.convnext = models.convnext_small(pretrained=True)\n",
        "\n",
        "        # Modify the classifier to exclude the last Linear layer\n",
        "        self.convnext.classifier = nn.Sequential(\n",
        "            self.convnext.classifier[0],  # LayerNorm\n",
        "            self.convnext.classifier[1],  # Flatten\n",
        "            # Exclude the last Linear layer\n",
        "        )\n",
        "\n",
        "        # Obtain num_ftrs from the classifier's LayerNorm layer\n",
        "        num_ftrs = self.convnext.classifier[0].normalized_shape[0]  # Should be 768\n",
        "\n",
        "        # Additional layer for hour feature(s)\n",
        "        self.hour_fc = nn.Sequential(\n",
        "            nn.Linear(num_hour_features, 16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Final layer for binary classification\n",
        "        self.final_fc = nn.Linear(num_ftrs + 16, 1)\n",
        "\n",
        "    def forward(self, images, hours):\n",
        "        # Forward pass through ConvNeXt\n",
        "        convnext_features = self.convnext(images)  # Output shape: [batch_size, num_ftrs]\n",
        "\n",
        "        # Forward pass through the hour feature layer\n",
        "        hour_features = self.hour_fc(hours)  # Output shape: [batch_size, 16]\n",
        "\n",
        "        # Combine the features\n",
        "        combined_features = torch.cat((convnext_features, hour_features), dim=1)  # Shape: [batch_size, num_ftrs + 16]\n",
        "\n",
        "        # Final classification\n",
        "        output = self.final_fc(combined_features)  # Output shape: [batch_size, 1]\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "MXtX55UJb2uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.01"
      ],
      "metadata": {
        "id": "LXZiz-IN-GTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ConvNextWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV1Kdfjd-Izs",
        "outputId": "420cbee2-5f51-413c-b1cf-4b3606f7136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Small_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6H_bAlu-KvD",
        "outputId": "0e847ea3-fc8c-4f8c-a633-63cc0ecb9992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.3836081624031067, Accuracy: 76.74258702191663%\n",
            "Iteration: 1000, Loss: 0.36731356382369995, Accuracy: 79.87967339922648%\n",
            "Iteration: 1500, Loss: 0.3940824270248413, Accuracy: 76.81993983669962%\n",
            "Iteration: 2000, Loss: 0.38143190741539, Accuracy: 80.05156854318865%\n",
            "Iteration: 2500, Loss: 0.3344791531562805, Accuracy: 80.32660077352814%\n",
            "Iteration: 3000, Loss: 0.27782735228538513, Accuracy: 79.87967339922648%\n",
            "Iteration: 3500, Loss: 0.33815547823905945, Accuracy: 80.0859475719811%\n",
            "Iteration: 4000, Loss: 0.5515509843826294, Accuracy: 78.85689729265148%\n",
            "Iteration: 4500, Loss: 0.4260536730289459, Accuracy: 80.12892135797163%\n",
            "Iteration: 5000, Loss: 0.45773574709892273, Accuracy: 80.12892135797163%\n",
            "Iteration: 5500, Loss: 0.34237709641456604, Accuracy: 81.22905027932961%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCDW50Uu-PWz",
        "outputId": "e2d1169d-2335-411d-996d-307328e67052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.3922\n",
            "Accuracy: 0.8035\n",
            "Precision: 0.8617\n",
            "Recall: 0.7642\n",
            "F1 Score: 0.8100\n",
            "ROC AUC Score: 0.8640\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_czmE3Dd-Sa7",
        "outputId": "8137c35e-725c-4279-9bcd-7bb16a28c855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 851330560 bytes\n",
            "Memory Reserved after model1 deletion: 2579496960 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.001"
      ],
      "metadata": {
        "id": "6HGVaYJ0auAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ConvNextWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYyBi4pdapQ7",
        "outputId": "461b24e4-878e-405b-bc6f-c456f3af58aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Small_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHa0Z2tda6m7",
        "outputId": "ca747687-3b32-41ca-d7da-16e427568be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.4274495840072632, Accuracy: 80.72195960464117%\n",
            "Iteration: 1000, Loss: 0.44885680079460144, Accuracy: 83.85904598195101%\n",
            "Iteration: 1500, Loss: 0.3224053382873535, Accuracy: 88.28534593897723%\n",
            "Iteration: 2000, Loss: 0.22669798135757446, Accuracy: 90.38246669531586%\n",
            "Iteration: 2500, Loss: 0.26580601930618286, Accuracy: 89.77223893425011%\n",
            "Iteration: 3000, Loss: 0.0857299193739891, Accuracy: 93.52814782982381%\n",
            "Iteration: 3500, Loss: 0.13706772029399872, Accuracy: 93.08122045552213%\n",
            "Iteration: 4000, Loss: 0.1726321280002594, Accuracy: 93.82896433175763%\n",
            "Iteration: 4500, Loss: 0.25839924812316895, Accuracy: 94.78298238074774%\n",
            "Iteration: 5000, Loss: 0.14148958027362823, Accuracy: 94.49935539321014%\n",
            "Iteration: 5500, Loss: 0.09929095953702927, Accuracy: 94.25010743446498%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDVc9I4UbAkW",
        "outputId": "64e03063-c3d4-466c-82fe-25ca7f8809c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.1362\n",
            "Accuracy: 0.9430\n",
            "Precision: 0.9740\n",
            "Recall: 0.9207\n",
            "F1 Score: 0.9466\n",
            "ROC AUC Score: 0.9867\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkA4FVcVbIK2",
        "outputId": "bf594c14-83c8-42a3-cd2d-a1709f2f5fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 852194816 bytes\n",
            "Memory Reserved after model1 deletion: 2350907392 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.0001"
      ],
      "metadata": {
        "id": "a1nS7ojodNz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ConvNextWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "LO68pFp3cAKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s7kq7C-dBs8",
        "outputId": "7d2ab45f-3a36-4e04-c5b7-050544da45a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.148818701505661, Accuracy: 94.49935539321014%\n",
            "Iteration: 1000, Loss: 0.1373743712902069, Accuracy: 94.74860335195531%\n",
            "Iteration: 1500, Loss: 0.14345936477184296, Accuracy: 95.83154275891707%\n",
            "Iteration: 2000, Loss: 0.17154520750045776, Accuracy: 96.11516974645467%\n",
            "Iteration: 2500, Loss: 0.0897204801440239, Accuracy: 96.08079071766223%\n",
            "Iteration: 3000, Loss: 0.11211910098791122, Accuracy: 95.59088955737%\n",
            "Iteration: 3500, Loss: 0.06653853505849838, Accuracy: 96.20971207563386%\n",
            "Iteration: 4000, Loss: 0.05959409102797508, Accuracy: 96.39020197679416%\n",
            "Iteration: 4500, Loss: 0.07267892360687256, Accuracy: 95.53932101418135%\n",
            "Iteration: 5000, Loss: 0.03533121943473816, Accuracy: 96.44177051998281%\n",
            "Iteration: 5500, Loss: 0.11817114055156708, Accuracy: 95.71121615814353%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdnXwHYudIsE",
        "outputId": "96d98fa9-8c44-475a-d43c-22737d57439f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.0933\n",
            "Accuracy: 0.9689\n",
            "Precision: 0.9646\n",
            "Recall: 0.9791\n",
            "F1 Score: 0.9718\n",
            "ROC AUC Score: 0.9937\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXJ_05F8dL3d",
        "outputId": "2ddfc717-8a01-46f2-e52d-d0462b21411f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 849681920 bytes\n",
            "Memory Reserved after model1 deletion: 2134900736 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning rate 0.00001"
      ],
      "metadata": {
        "id": "ciL5jJCF-X4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ConvNextWithHours(num_hour_features=1).to(device)\n",
        "# Calculate pos_weight and initialize BCEWithLogitsLoss with pos_weight\n",
        "negatives = 34689\n",
        "positives = 42869\n",
        "pos_weight = torch.tensor([negatives / positives]).to(device)\n",
        "error = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "# Initial setup as before\n",
        "\n",
        "\n",
        "total_epochs = 7\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZUgrM0-dtJ",
        "outputId": "fe710211-af41-4c21-ccbc-050a861da865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Small_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Lists for visualization of loss and accuracy\n",
        "loss_list = []\n",
        "iteration_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "# Assume optimizer and model are properly initialized\n",
        "for epoch in range(total_epochs):\n",
        "    for images, labels, hours in train_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        count += 1\n",
        "\n",
        "        # Testing the model\n",
        "        if not (count % 50):  # Evaluate every 50 iterations\n",
        "            total = 0\n",
        "            correct = 0\n",
        "\n",
        "            # No gradient tracking needed for evaluation\n",
        "            with torch.no_grad():\n",
        "                for images, labels, hours in test_loader:\n",
        "                    labels = labels.float()\n",
        "                    images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "                    outputs = model(images, hours)\n",
        "                    predicted = torch.sigmoid(outputs).round()\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += (predicted == labels).sum().item()  # Ensure labels are correctly shaped\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            loss_list.append(loss.item())\n",
        "            iteration_list.append(count)\n",
        "            accuracy_list.append(accuracy)\n",
        "\n",
        "            if not (count % 500):  # Print every 500 iterations\n",
        "                print(f\"Iteration: {count}, Loss: {loss.item()}, Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJrzHyHV-jnV",
        "outputId": "84f3174a-c801-4916-996f-102a6ae40f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 500, Loss: 0.2968873679637909, Accuracy: 93.31327889987108%\n",
            "Iteration: 1000, Loss: 0.07650311291217804, Accuracy: 95.62526858616243%\n",
            "Iteration: 1500, Loss: 0.040418121963739395, Accuracy: 95.3072625698324%\n",
            "Iteration: 2000, Loss: 0.058741066604852676, Accuracy: 95.71121615814353%\n",
            "Iteration: 2500, Loss: 0.07974477112293243, Accuracy: 96.02922217447357%\n",
            "Iteration: 3000, Loss: 0.0561225488781929, Accuracy: 96.35582294800172%\n",
            "Iteration: 3500, Loss: 0.08789858222007751, Accuracy: 96.3300386764074%\n",
            "Iteration: 4000, Loss: 0.14374113082885742, Accuracy: 96.42458100558659%\n",
            "Iteration: 4500, Loss: 0.0663643330335617, Accuracy: 96.57928663515256%\n",
            "Iteration: 5000, Loss: 0.1284179985523224, Accuracy: 96.613665663945%\n",
            "Iteration: 5500, Loss: 0.03941658139228821, Accuracy: 96.59647614954878%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "all_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels, hours in val_loader:\n",
        "        labels = labels.float()\n",
        "        images, labels, hours = images.to(device), labels.to(device), hours.to(device)\n",
        "\n",
        "        outputs = model(images, hours)\n",
        "        loss = error(outputs, labels)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        predicted = probabilities.round()\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().flatten())\n",
        "        all_predictions.extend(predicted.cpu().numpy().flatten())\n",
        "        all_probabilities.extend(probabilities.cpu().numpy().flatten())\n",
        "\n",
        "# Average validation loss\n",
        "val_loss /= len(val_loader)\n",
        "\n",
        "# Compute metrics\n",
        "precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
        "recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
        "f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "auc = roc_auc_score(all_labels, all_probabilities)\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(f\"\\nValidation Set Performance:\")\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"ROC AUC Score: {auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-_exV_4-l4N",
        "outputId": "22970802-c856-4681-a010-8a05999aeb4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Performance:\n",
            "Validation Loss: 0.0903\n",
            "Accuracy: 0.9679\n",
            "Precision: 0.9669\n",
            "Recall: 0.9749\n",
            "F1 Score: 0.9709\n",
            "ROC AUC Score: 0.9927\n",
            "Confusion Matrix:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete model1 and associated variables\n",
        "del model\n",
        "del optimizer\n",
        "#del scheduler  # if you used one\n",
        "del loss  # if stored\n",
        "\n",
        "# Clear any cached data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# (Optional) Verify GPU memory usage\n",
        "print(f\"Memory Allocated after model1 deletion: {torch.cuda.memory_allocated()} bytes\")\n",
        "print(f\"Memory Reserved after model1 deletion: {torch.cuda.memory_reserved()} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stRfT4qn-mim",
        "outputId": "266d6c0e-0114-4afe-c60c-ea0eceacad7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory Allocated after model1 deletion: 850928128 bytes\n",
            "Memory Reserved after model1 deletion: 2327838720 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plot**"
      ],
      "metadata": {
        "id": "2FKT7WiqzAhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Your data\n",
        "model_name = ['VGG-16', 'ResNet18', 'ResNet50', 'EfficientNet', 'ConvNeXt']\n",
        "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
        "\n",
        "accuracy_1 = [80.30, 94.51, 94.98, 95.74]       # For VGG-16\n",
        "accuracy_2 = [93.94, 95.62, 94.27, 95.62]       # For ResNet18\n",
        "accuracy_3 = [94.21, 93.66, 96.31, 95.80]       # For ResNet50\n",
        "accuracy_4 = [94.79, 96.53, 96.30, 94.78]       # For EfficientNet\n",
        "accuracy_5 = [80.35, 94.30, 96.89, 96.79]       # For ConvNeXt\n",
        "\n",
        "# Create a DataFrame for accuracy\n",
        "accuracies = [accuracy_1, accuracy_2, accuracy_3, accuracy_4, accuracy_5]\n",
        "df_accuracy = pd.DataFrame(accuracies, index=model_name, columns=learning_rates)\n",
        "\n",
        "# Similarly for F1 scores\n",
        "F1_1 = [0.8084, 0.9508, 0.9528, 0.9606]\n",
        "F1_2 = [0.9471, 0.9602, 0.9464, 0.9609]\n",
        "F1_3 = [0.9480, 0.9463, 0.9665, 0.9617]\n",
        "F1_4 = [0.9540, 0.9687, 0.9667, 0.9532]\n",
        "F1_5 = [0.8100, 0.9466, 0.9718, 0.9709]\n",
        "\n",
        "# Convert F1 scores to percentages for consistency\n",
        "F1_scores = [[score * 100 for score in lst] for lst in [F1_1, F1_2, F1_3, F1_4, F1_5]]\n",
        "df_F1 = pd.DataFrame(F1_scores, index=model_name, columns=learning_rates)\n",
        "df_accuracy.columns = ['1e-2', '1e-3', '1e-4', '1e-5']\n",
        "df_F1.columns = ['1e-2', '1e-3', '1e-4', '1e-5']\n",
        "sns.set_context(\"talk\", font_scale=1.2)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Plot Accuracy Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_accuracy, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5, linecolor='gray')\n",
        "plt.xlabel('Learning Rate', fontsize=18)\n",
        "plt.title('Model Accuracy Heatmap (%)', fontsize=18)\n",
        "plt.yticks(rotation=0, fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot F1 Score Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df_F1, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5, linecolor='gray')\n",
        "plt.xlabel('Learning Rate', fontsize=18)\n",
        "plt.title('Model F1 Score Heatmap (%)', fontsize=18)\n",
        "plt.yticks(rotation=0, fontsize=16)\n",
        "plt.xticks(rotation=45, fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X6CTO2oQzIdt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}